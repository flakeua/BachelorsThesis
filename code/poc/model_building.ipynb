{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../datasets/cgds_2/'\n",
    "imagePaths = sorted(list(map(lambda x: os.path.join(dataset_path,x), filter(lambda x: x not in ['', '.DS_Store'], os.listdir(dataset_path)))))\n",
    "\n",
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "INPUT_IMAGE_HEIGHT = 300\n",
    "INPUT_IMAGE_WIDTH = 900\n",
    "device = \"mps:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.ToPILImage(),\n",
    " \ttransforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)),\n",
    "\ttransforms.ToTensor()])\n",
    "\n",
    "class EyeDataset(Dataset):\n",
    "\tdef __init__(self, imagePaths, transforms=None):\n",
    "\t\tself.imagePaths = imagePaths\n",
    "\t\tself.transforms = transforms\n",
    "\tdef __len__(self):\n",
    "\t\t# return the number of total samples contained in the dataset\n",
    "\t\treturn len(self.imagePaths)\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# grab the image path from the current index\n",
    "\t\timagePath = self.imagePaths[idx]\n",
    "\t\t# load the image from disk, swap its channels from BGR to RGB,\n",
    "\t\t# and read the associated mask from disk in grayscale mode\n",
    "\t\timage = cv2.imread(imagePath)\n",
    "\t\tres = imagePath.split('/')[-1].split('.jpg')[0].split('_')\n",
    "\t\tpitch = res[2][:-1]\n",
    "\t\tyaw = res[3][:-1]\n",
    "\t\tres = [float(pitch), float(yaw)]\n",
    "\t\tres = torch.tensor(res, dtype=torch.float32)\n",
    "\t\tres = res.to(device)\n",
    "\t\t# check to see if we are applying any transformations\n",
    "\t\tif self.transforms is not None:\n",
    "\t\t\timage = self.transforms(image)\n",
    "\t\timage = torch.tensor(image).to(device)\n",
    "\t\t# return a tuple of the image and its mask\n",
    "\t\treturn (image, res)\n",
    "\n",
    "dataset = EyeDataset(imagePaths, transforms)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 4704 examples in the training set...\n",
      "[INFO] found 1176 examples in the test set...\n"
     ]
    }
   ],
   "source": [
    "(trainImages, testImages) = train_test_split(imagePaths, test_size=TEST_SPLIT, random_state=42)\n",
    "\n",
    "# create the train and test datasets\n",
    "trainDS = EyeDataset(imagePaths=trainImages, transforms=transforms)\n",
    "testDS = EyeDataset(imagePaths=testImages, transforms=transforms)\n",
    "\n",
    "print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
    "print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
    "\n",
    "# create the training and test data loaders\n",
    "trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "\tbatch_size=BATCH_SIZE, num_workers=0)\n",
    "testLoader = DataLoader(testDS, shuffle=False,\n",
    "\tbatch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstEyeNet(nn.ModuleList):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 10)\n",
    "        self.pool = nn.MaxPool2d(3, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 10)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 10)\n",
    "        self.fc1 = nn.Linear(5568, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = FirstEyeNet()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/lsyp9lq122g_dcsz7sxdd7x00000gp/T/ipykernel_60274/4255805324.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.847\n",
      "[1,    40] loss: 0.843\n",
      "[1,    60] loss: 0.809\n",
      "[1,    80] loss: 0.804\n",
      "[1,   100] loss: 0.736\n",
      "[1,   120] loss: 0.743\n",
      "[1,   140] loss: 0.728\n",
      "[2,    20] loss: 0.709\n",
      "[2,    40] loss: 0.696\n",
      "[2,    60] loss: 0.702\n",
      "[2,    80] loss: 0.697\n",
      "[2,   100] loss: 0.690\n",
      "[2,   120] loss: 0.671\n",
      "[2,   140] loss: 0.669\n",
      "[3,    20] loss: 0.636\n",
      "[3,    40] loss: 0.602\n",
      "[3,    60] loss: 0.607\n",
      "[3,    80] loss: 0.597\n",
      "[3,   100] loss: 0.567\n",
      "[3,   120] loss: 0.565\n",
      "[3,   140] loss: 0.585\n",
      "[4,    20] loss: 0.551\n",
      "[4,    40] loss: 0.488\n",
      "[4,    60] loss: 0.418\n",
      "[4,    80] loss: 0.468\n",
      "[4,   100] loss: 0.457\n",
      "[4,   120] loss: 0.465\n",
      "[4,   140] loss: 0.462\n",
      "[5,    20] loss: 0.363\n",
      "[5,    40] loss: 0.367\n",
      "[5,    60] loss: 0.340\n",
      "[5,    80] loss: 0.328\n",
      "[5,   100] loss: 0.344\n",
      "[5,   120] loss: 0.333\n",
      "[5,   140] loss: 0.315\n",
      "[6,    20] loss: 0.270\n",
      "[6,    40] loss: 0.248\n",
      "[6,    60] loss: 0.279\n",
      "[6,    80] loss: 0.240\n",
      "[6,   100] loss: 0.259\n",
      "[6,   120] loss: 0.220\n",
      "[6,   140] loss: 0.251\n",
      "[7,    20] loss: 0.208\n",
      "[7,    40] loss: 0.174\n",
      "[7,    60] loss: 0.162\n",
      "[7,    80] loss: 0.179\n",
      "[7,   100] loss: 0.173\n",
      "[7,   120] loss: 0.170\n",
      "[7,   140] loss: 0.180\n",
      "[8,    20] loss: 0.135\n",
      "[8,    40] loss: 0.143\n",
      "[8,    60] loss: 0.129\n",
      "[8,    80] loss: 0.143\n",
      "[8,   100] loss: 0.135\n",
      "[8,   120] loss: 0.142\n",
      "[8,   140] loss: 0.154\n",
      "[9,    20] loss: 0.100\n",
      "[9,    40] loss: 0.109\n",
      "[9,    60] loss: 0.095\n",
      "[9,    80] loss: 0.095\n",
      "[9,   100] loss: 0.099\n",
      "[9,   120] loss: 0.100\n",
      "[9,   140] loss: 0.095\n",
      "[10,    20] loss: 0.072\n",
      "[10,    40] loss: 0.074\n",
      "[10,    60] loss: 0.072\n",
      "[10,    80] loss: 0.072\n",
      "[10,   100] loss: 0.081\n",
      "[10,   120] loss: 0.076\n",
      "[10,   140] loss: 0.081\n",
      "[11,    20] loss: 0.057\n",
      "[11,    40] loss: 0.056\n",
      "[11,    60] loss: 0.060\n",
      "[11,    80] loss: 0.071\n",
      "[11,   100] loss: 0.060\n",
      "[11,   120] loss: 0.064\n",
      "[11,   140] loss: 0.058\n",
      "[12,    20] loss: 0.049\n",
      "[12,    40] loss: 0.047\n",
      "[12,    60] loss: 0.042\n",
      "[12,    80] loss: 0.051\n",
      "[12,   100] loss: 0.045\n",
      "[12,   120] loss: 0.052\n",
      "[12,   140] loss: 0.054\n",
      "[13,    20] loss: 0.036\n",
      "[13,    40] loss: 0.035\n",
      "[13,    60] loss: 0.038\n",
      "[13,    80] loss: 0.037\n",
      "[13,   100] loss: 0.034\n",
      "[13,   120] loss: 0.037\n",
      "[13,   140] loss: 0.038\n",
      "[14,    20] loss: 0.028\n",
      "[14,    40] loss: 0.028\n",
      "[14,    60] loss: 0.036\n",
      "[14,    80] loss: 0.031\n",
      "[14,   100] loss: 0.033\n",
      "[14,   120] loss: 0.033\n",
      "[14,   140] loss: 0.030\n",
      "[15,    20] loss: 0.027\n",
      "[15,    40] loss: 0.030\n",
      "[15,    60] loss: 0.035\n",
      "[15,    80] loss: 0.035\n",
      "[15,   100] loss: 0.028\n",
      "[15,   120] loss: 0.027\n",
      "[15,   140] loss: 0.026\n",
      "[16,    20] loss: 0.019\n",
      "[16,    40] loss: 0.022\n",
      "[16,    60] loss: 0.020\n",
      "[16,    80] loss: 0.022\n",
      "[16,   100] loss: 0.022\n",
      "[16,   120] loss: 0.025\n",
      "[16,   140] loss: 0.029\n",
      "[17,    20] loss: 0.025\n",
      "[17,    40] loss: 0.022\n",
      "[17,    60] loss: 0.022\n",
      "[17,    80] loss: 0.020\n",
      "[17,   100] loss: 0.020\n",
      "[17,   120] loss: 0.021\n",
      "[17,   140] loss: 0.020\n",
      "[18,    20] loss: 0.017\n",
      "[18,    40] loss: 0.019\n",
      "[18,    60] loss: 0.018\n",
      "[18,    80] loss: 0.016\n",
      "[18,   100] loss: 0.016\n",
      "[18,   120] loss: 0.015\n",
      "[18,   140] loss: 0.017\n",
      "[19,    20] loss: 0.013\n",
      "[19,    40] loss: 0.015\n",
      "[19,    60] loss: 0.014\n",
      "[19,    80] loss: 0.016\n",
      "[19,   100] loss: 0.017\n",
      "[19,   120] loss: 0.016\n",
      "[19,   140] loss: 0.015\n",
      "[20,    20] loss: 0.011\n",
      "[20,    40] loss: 0.013\n",
      "[20,    60] loss: 0.012\n",
      "[20,    80] loss: 0.012\n",
      "[20,   100] loss: 0.013\n",
      "[20,   120] loss: 0.013\n",
      "[20,   140] loss: 0.016\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainLoader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './models/first_eye_model.pth'\n",
    "# torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/lsyp9lq122g_dcsz7sxdd7x00000gp/T/ipykernel_60274/4255805324.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 18.564542770385742, 0.038050174713134766, 2.7281651496887207\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "criterion2 = nn.L1Loss().to(device)\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        losses.append(criterion2(outputs, labels))\n",
    "\n",
    "        \n",
    "print(f'Accuracy of the network on the test images: {max(losses)}, {min(losses)}, {sum(losses)/len(losses)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365bfb8af186f2789ca12ea7dcf8977d7b96ddfeb0cb1cd79b71fb55098bbcb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
