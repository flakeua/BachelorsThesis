{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch import NeuralNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../datasets/cgds_2_m/'\n",
    "imagePaths = sorted(list(map(lambda x: os.path.join(dataset_path,x), filter(lambda x: x not in ['', '.DS_Store'], os.listdir(dataset_path)))))\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "BATCH_SIZE = 64\n",
    "INPUT_IMAGE_HEIGHT = 100\n",
    "INPUT_IMAGE_WIDTH = 300\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.ToPILImage(),\n",
    " \ttransforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)),\n",
    "\ttransforms.ToTensor()])\n",
    "\n",
    "class EyeDataset(Dataset):\n",
    "\tdef __init__(self, imagePaths, transforms=None):\n",
    "\t\tself.imagePaths = imagePaths\n",
    "\t\tself.transforms = transforms\n",
    "\tdef __len__(self):\n",
    "\t\t# return the number of total samples contained in the dataset\n",
    "\t\treturn len(self.imagePaths)\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# grab the image path from the current index\n",
    "\t\timagePath = self.imagePaths[idx]\n",
    "\t\t# load the image from disk, swap its channels from BGR to RGB,\n",
    "\t\t# and read the associated mask from disk in grayscale mode\n",
    "\t\timage = cv2.imread(imagePath)\n",
    "\t\tres = imagePath.split('/')[-1].split('.jpg')[0].split('_')\n",
    "\t\tpitch = res[2][:-1]\n",
    "\t\tyaw = res[3][:-1]\n",
    "\t\thp = res[4]\n",
    "\t\thr = res[5]\n",
    "\t\thy = res[6]\n",
    "\t\thead_pos = torch.tensor([float(hp)/180, float(hr)/180, float(hy)/180], dtype=torch.float32).to(device)\n",
    "\t\tres = [float(pitch), float(yaw)]\n",
    "\t\tres = torch.tensor(res, dtype=torch.float32)\n",
    "\t\tres = res.to(device)\n",
    "\t\t# check to see if we are applying any transformations\n",
    "\t\tif self.transforms is not None:\n",
    "\t\t\timage = self.transforms(image)\n",
    "\t\timage = torch.tensor(image).to(device)\n",
    "\t\t# return a tuple of the image and its mask\n",
    "\t\treturn ((image, head_pos), res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 10585 examples in the training set...\n",
      "[INFO] found 1177 examples in the test set...\n"
     ]
    }
   ],
   "source": [
    "(trainImages, testImages) = train_test_split(imagePaths, test_size=TEST_SPLIT, random_state=42)\n",
    "\n",
    "# create the train and test datasets\n",
    "trainDS = EyeDataset(imagePaths=trainImages, transforms=transforms)\n",
    "testDS = EyeDataset(imagePaths=testImages, transforms=transforms)\n",
    "\n",
    "# sklearn dataset\n",
    "# ds = EyeDataset(imagePaths=imagePaths, transforms=transforms)\n",
    "# X, y = list(map(lambda x: x[0], ds)), list(map(lambda x: x[1], ds))\n",
    "\n",
    "print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
    "print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
    "\n",
    "# create the training and test data loaders\n",
    "trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "\tbatch_size=BATCH_SIZE, num_workers=0)\n",
    "testLoader = DataLoader(testDS, shuffle=False,\n",
    "\tbatch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThirdEyeNet(nn.ModuleList):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 9, 10)\n",
    "        self.pool = nn.MaxPool2d(3, 3)\n",
    "        self.conv2 = nn.Conv2d(9, 26, 10)\n",
    "        self.fc1 = nn.Linear(5281, 700)\n",
    "        self.fc2 = nn.Linear(700, 130)\n",
    "        self.fc3 = nn.Linear(130, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, head_pos = x\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = torch.cat((x, head_pos), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# net = NeuralNetRegressor(FirstEyeNet, max_epochs = 100, lr=0.001, verbose=1, criterion = nn.MSELoss().to(device))\n",
    "# net.device = 'mps:0'\n",
    "\n",
    "net = ThirdEyeNet()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/lsyp9lq122g_dcsz7sxdd7x00000gp/T/ipykernel_60427/707303263.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 82.928\n",
      "[1,    40] loss: 78.043\n",
      "[1,    60] loss: 71.461\n",
      "[1,    80] loss: 71.125\n",
      "[1,   100] loss: 68.903\n",
      "[1,   120] loss: 62.351\n",
      "[1,   140] loss: 60.150\n",
      "[1,   160] loss: 58.606\n",
      "[2,    20] loss: 58.621\n",
      "[2,    40] loss: 52.548\n",
      "[2,    60] loss: 47.553\n",
      "[2,    80] loss: 44.066\n",
      "[2,   100] loss: 42.746\n",
      "[2,   120] loss: 40.559\n",
      "[2,   140] loss: 39.230\n",
      "[2,   160] loss: 35.759\n",
      "[3,    20] loss: 36.316\n",
      "[3,    40] loss: 32.326\n",
      "[3,    60] loss: 30.516\n",
      "[3,    80] loss: 28.752\n",
      "[3,   100] loss: 26.759\n",
      "[3,   120] loss: 27.085\n",
      "[3,   140] loss: 28.241\n",
      "[3,   160] loss: 27.650\n",
      "[4,    20] loss: 24.053\n",
      "[4,    40] loss: 22.650\n",
      "[4,    60] loss: 22.903\n",
      "[4,    80] loss: 21.888\n",
      "[4,   100] loss: 22.108\n",
      "[4,   120] loss: 20.425\n",
      "[4,   140] loss: 19.166\n",
      "[4,   160] loss: 17.626\n",
      "[5,    20] loss: 17.145\n",
      "[5,    40] loss: 15.172\n",
      "[5,    60] loss: 16.794\n",
      "[5,    80] loss: 15.971\n",
      "[5,   100] loss: 16.653\n",
      "[5,   120] loss: 14.152\n",
      "[5,   140] loss: 14.725\n",
      "[5,   160] loss: 14.799\n",
      "Accuracy of the network on the test images: 15.09868049621582, 0.06131935119628906, 2.972898006439209\n",
      "[6,    20] loss: 10.797\n",
      "[6,    40] loss: 11.722\n",
      "[6,    60] loss: 12.145\n",
      "[6,    80] loss: 12.263\n",
      "[6,   100] loss: 12.725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [124], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m every \u001b[39m==\u001b[39m every \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:    \u001b[39m# print every 2000 mini-batches\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m5d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] loss: \u001b[39m\u001b[39m{\u001b[39;00mrunning_loss \u001b[39m/\u001b[39m every\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "changed = False\n",
    "changed2 = False\n",
    "every = 20\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainLoader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % every == every - 1:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / every:.3f}')\n",
    "            if running_loss / every < 2 and not changed: \n",
    "                changed = True\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] *= 0.1\n",
    "            if running_loss / every < 0.25 and not changed2: \n",
    "                changed2 = True\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] *= 0.1\n",
    "            running_loss = 0.0\n",
    "    losses = []\n",
    "    if epoch % 5 == 4:    # print every 2000 mini-batches\n",
    "        criterion2 = nn.L1Loss().to(device)\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for data in testLoader:\n",
    "                images, labels = data\n",
    "                # calculate outputs by running images through the network\n",
    "                outputs = net(images)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                losses.append(criterion2(outputs, labels))\n",
    "        print(f'Accuracy of the network on the test images: {max(losses)}, {min(losses)}, {sum(losses)/len(losses)}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     ('scale', StandardScaler()),\n",
    "#     ('net', net),\n",
    "# ])\n",
    "# X = torch.stack(X)\n",
    "# y = torch.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# # deactivate skorch-internal train-valid split and verbose logging\n",
    "# net.set_params(train_split=False, verbose=0)\n",
    "# params = {\n",
    "#     'lr': [0.001, 0.01],\n",
    "#     'max_epochs': [10],\n",
    "# }\n",
    "# gs = GridSearchCV(net, params, refit=False, cv=3,\n",
    "#                   scoring='accuracy', verbose=2)\n",
    "\n",
    "# gs.fit(X, y)\n",
    "# print(\"best score: {:.3f}, best params: {}\".format(\n",
    "#     gs.best_score_, gs.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './models/second_eye_model_mirrored_hv.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/lsyp9lq122g_dcsz7sxdd7x00000gp/T/ipykernel_60427/707303263.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 14.57371711730957, 0.033048465847969055, 1.6739957332611084\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "criterion2 = nn.L1Loss().to(device)\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        losses.append(criterion2(outputs, labels))\n",
    "\n",
    "        \n",
    "print(f'Accuracy of the network on the test images: {max(losses)}, {min(losses)}, {sum(losses)/len(losses)}')\n",
    "# Accuracy of the network on the test images: 11.710856437683105, 0.05646705627441406, 1.7454159259796143"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365bfb8af186f2789ca12ea7dcf8977d7b96ddfeb0cb1cd79b71fb55098bbcb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
